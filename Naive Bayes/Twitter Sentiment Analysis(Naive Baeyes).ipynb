{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baeyes Algorithm\n",
    "It is a very simple classificiation based on **Baeyes Theorem**.\n",
    "Assumptions:\n",
    "- ### IID:\n",
    "It states that all the features which are used in classification are independent and identical from each other.\n",
    "    - **Independent**: This means that each feature is purely independent of each other and one does not effect another.\n",
    "    - **Identical**: This means that we are assuming that each of the feature is obtained from the same distribution , in this case Mullitvariate Gaussian.\n",
    "#### Baeyes Theorem:\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "For machine learning , naive baeyes can be written as :\n",
    "$$P(class|data) = \\frac{P(data|class)P(class)}{P(data)}$$\n",
    "- P(class) : Prior Probability , it is the probability of class in the given dataset.\n",
    "- P(data): Total probability \n",
    "- P(data|clas) : Likelihood Probability , probability of class having a given data value associated to it.\n",
    "- P(class| data) : Posterior Probability , probability of data having a particular class\n",
    "\n",
    "![](https://i.ytimg.com/vi/eho8xH3E6mE/hqdefault.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_naive_baeyes import process_tweet, lookup\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    for y,tweet in zip(ys,tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word,y)\n",
    "            if pair in result:\n",
    "                result[pair]+=1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use log likelihood probability??\n",
    "In this type of data where each value is categorical and using any pdf is impossible then we use \\\n",
    " the probabilities calculated and use them instead to calculate Likelihood Probability. \\\n",
    "**BUT**\\\n",
    "Multiplying so many values will reach beyond the capability of python(lowest value possible is of 10^(-36) so we will calculate the probabilties by converting them into **Negative Log Probability** .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
    "The prior is the ratio of the probabilities :\n",
    "$\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
    "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$\n",
    "\n",
    "\n",
    "Positive Probability and negative probability for each word\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}$$\n",
    "Log Likeli hood :\n",
    "$$log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    \n",
    "    N_pos=N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        if pair[1] >0:\n",
    "            N_pos+= freqs[pair]\n",
    "        else:\n",
    "            N_neg+= freqs[pair]\n",
    "    D = len(train_y)\n",
    "    D_pos = sum(i==1 for i in train_y)\n",
    "    D_neg = D - D_pos\n",
    "    \n",
    "    # logprior \n",
    "    logprior = np.log(D_pos/D) - np.log(D_neg/D)\n",
    "    \n",
    "    for word in vocab:\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "        \n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_positive = (freq_pos+1)/(N_pos + V)\n",
    "        p_w_negative = (freq_neg + 1)/(N_neg + V)\n",
    "        \n",
    "        loglikelihood[word] = np.log(p_w_positive) - np.log(p_w_negative)\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9085\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediciting the porbability\n",
    "Probability will be calculated by adding logprior and loglikelihood probability. \n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    words = process_tweet(tweet)\n",
    "    \n",
    "    p = 0\n",
    "    p+=logprior\n",
    "    \n",
    "    for word in words:\n",
    "        if word in loglikelihood:\n",
    "            p+=loglikelihood[word]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 1.5737794405738939\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the algorithm \n",
    "- In this we will simply calculate the value of probability and then compare if it is greater than 0 then it assigned class '1' otherwise class '0'.\n",
    "- Then simply subtracting it by test_y and getting the mean absolute value for this error and subtracting this error from 1 to get our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    \n",
    "    y_hats = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        if naive_bayes_predict(tweet,logprior,loglikelihood) > 0:\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            y_hat_i = 0\n",
    "        y_hats.append(y_hat_i)\n",
    "        \n",
    "    error = np.mean(np.absolute(y_hats - test_y))\n",
    "    \n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9940\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 2.15\n",
      "I am bad -> -1.29\n",
      "this movie should have been great. -> 2.14\n",
      "great -> 2.14\n",
      "great great -> 4.28\n",
      "great great great -> 6.41\n",
      "great great great great -> 8.55\n"
     ]
    }
   ],
   "source": [
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.802119484044239"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the ratio\n",
    "- Sometimes there are words which have greater impact on deciding the class of the tweet,the so called 'power words'.\n",
    "- To get these power words we will calculate the ratio and also compare them with some threshold afterwards.\n",
    "$$ ratio = \\frac{\\text{pos_words} + 1}{\\text{neg_words} + 1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(freqs,word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "        word: string to lookup\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive':0 , 'negative':0 ,'ratio':0.0}\n",
    "    pos_neg_ratio['positive'] = lookup(freqs,word,1)\n",
    "    pos_neg_ratio['negative'] = lookup(freqs,word,0)\n",
    "    \n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)\n",
    "    \n",
    "    return pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 161, 'negative': 18, 'ratio': 8.526315789473685}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs, 'happi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_by_threshold(freqs, label, threshold):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary of words\n",
    "        label: 1 for positive, 0 for negative\n",
    "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
    "    Output:\n",
    "        word_set: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
    "        example of a key value pair:\n",
    "        {'happi':\n",
    "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "        }\n",
    "    '''\n",
    "    word_list = {}\n",
    "    \n",
    "    for key in freqs.keys():\n",
    "        word,_ = key\n",
    "        \n",
    "        pos_neg_ratio = get_ratio(freqs,word)\n",
    "        \n",
    "        if label == 1 and pos_neg_ratio['ratio']>=threshold:\n",
    "            word_list[word] = pos_neg_ratio\n",
    "        elif label == 0 and pos_neg_ratio['ratio']<=threshold:\n",
    "            word_list[word] = pos_neg_ratio\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{':(': {'positive': 1, 'negative': 3663, 'ratio': 0.0005458515283842794},\n",
       " ':-(': {'positive': 0, 'negative': 378, 'ratio': 0.002638522427440633},\n",
       " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '26': {'positive': 0, 'negative': 20, 'ratio': 0.047619047619047616},\n",
       " '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728},\n",
       " 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '♛': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " '》': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " 'beli̇ev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'wi̇ll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'justi̇n': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｓｅｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｍｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_by_threshold(freqs, label=0, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
       " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
       " ':)': {'positive': 2847, 'negative': 2, 'ratio': 949.3333333333334},\n",
       " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':D': {'positive': 498, 'negative': 0, 'ratio': 499.0},\n",
       " ':p': {'positive': 103, 'negative': 0, 'ratio': 104.0},\n",
       " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':-)': {'positive': 543, 'negative': 0, 'ratio': 544.0},\n",
       " \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
       " 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
       " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
       " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
       " 'via': {'positive': 60, 'negative': 1, 'ratio': 30.5},\n",
       " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
       " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
       " 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
       " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_by_threshold(freqs, label=1, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6636283173328446\n"
     ]
    }
   ],
   "source": [
    "# It is a positive sentiment as it is true 😉\n",
    "my_tweet = 'Dc is better than marvels'\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
